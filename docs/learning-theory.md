# Learning theory

## Statistical Learning Theory

## Deep Learning Theory
+ **Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks** *(ICLR, 2020)* [[paper]](https://arxiv.org/abs/1909.12292)  
  Ziwei Ji, Matus Telgarsky



## Multi-distribution Learning
+ **On-Demand Sampling: Learning Optimally from Multiple Distributions** *(NeurIPS, 2022)* [[paper]](https://arxiv.org/abs/2210.12529) [[code]](https://github.com/ericzhao28/multidistributionlearning) [[notes]](https://github.com/RaphelWei/ml_reading/issues/1)   
  Nika Haghtalab, Michael I. Jordan, Eric Zhao

+ **Open Problem: The Sample Complexity of Multi-Distribution
Learning for VC Classes** *(COLT, 2023)* [[paper]](https://arxiv.org/abs/2307.12135) [[notes]](https://github.com/RaphelWei/ml_reading/issues/2)  
  Pranjal Awasthi, Nika Haghtalab, Eric Zhao.

+ **Optimal Multi-Distribution Learning** *(COLT, 2024)* [[paper]](https://arxiv.org/abs/2312.05134) [[notes]](https://github.com/RaphelWei/ml_reading/issues/3)  
  Zihan Zhang, Wenhao Zhan, Yuxin Chen, Simon S. Du, Jason D. Lee

+ **The sample complexity of multi-distribution learning** *(COLT, 2024)* [[paper]](https://arxiv.org/abs/2312.04027)   
  Binghui Peng