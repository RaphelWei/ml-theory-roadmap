# Learning theory

Covers classical and deep learning theory, from generalization bounds to optimization-driven implicit bias.

## Statistical Learning Theory

This section covers classic generalization theory under the i.i.d. assumption, including PAC learning, VC dimension, Rademacher complexity, and algorithmic stability.

+ **Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD** *(ICLR, 2023)* [[paper]](https://arxiv.org/abs/2204.12446)  
  Konstantinos E. Nikolakakis, Farzin Haddadpour, Amin Karbasi, Dionysios S. Kalogerias

## Deep Learning Theory

This section focuses on the theoretical foundations of deep neural networks, including optimization, generalization, and approximation.

+ **Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks** *(ICLR, 2020)* [[paper]](https://arxiv.org/abs/1909.12292)  
  Ziwei Ji, Matus Telgarsky

## Multi-distribution Learning

(Also known as "Collaborative PAC Learning", "Group Distributionally Robust optimization". See also GDRO in [Distributionally Robust Optimization](optimization.md#distributionally-robust-optimization))

+ **Derandomizing Multi-Distribution Learning** *(NeurIPS, 2024)* [[paper]](https://arxiv.org/abs/2409.17567)  
  Kasper Green Larsen, Omar Montasser, Nikita Zhivotovskiy

+ **Collaborative Learning with Different Labeling Functions** *(ICML, 2024)* [[paper]](https://arxiv.org/abs/2402.10445)  
  Yuyang Deng, Mingda Qiao

+ **Optimal Multi-Distribution Learning** *(COLT, 2024)* [[paper]](https://arxiv.org/abs/2312.05134) [[notes]](https://github.com/RaphelWei/ml_reading/issues/3)  
  Zihan Zhang, Wenhao Zhan, Yuxin Chen, Simon S. Du, Jason D. Lee

+ **The sample complexity of multi-distribution learning** *(COLT, 2024)* [[paper]](https://arxiv.org/abs/2312.04027)  
  Binghui Peng

+ **Open Problem: The Sample Complexity of Multi-Distribution
Learning for VC Classes** *(COLT, 2023)* [[paper]](https://arxiv.org/abs/2307.12135) [[notes]](https://github.com/RaphelWei/ml_reading/issues/2)  
  Pranjal Awasthi, Nika Haghtalab, Eric Zhao.

+ **On-Demand Sampling: Learning Optimally from Multiple Distributions** *(NeurIPS, 2022)* [[paper]](https://arxiv.org/abs/2210.12529) [[code]](https://github.com/ericzhao28/multidistributionlearning) [[notes]](https://github.com/RaphelWei/ml_reading/issues/1)  
  Nika Haghtalab, Michael I. Jordan, Eric Zhao

+ **Improved Algorithms for Collaborative PAC Learning** *(NeurIPS, 2018)* [[paper]](https://arxiv.org/abs/1805.08356)  
  Huy L. Nguyen, Lydia Zakynthinou

+ **Collaborative PAC Learning** *(NeurIPS, 2017)* [[paper]](https://papers.nips.cc/paper_files/paper/2017/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html)  
  Avrim Blum, Nika Haghtalab, Ariel D Procaccia, Mingda Qiao
